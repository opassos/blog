{
  
    
        "post0": {
            "title": "Construindo um modelo de Regressão Linear do zero",
            "content": ". !pip install -Uqqq matplotlib . Dados . Dados de número de vendedores e quantidade vendida (média diária) para 10 lojas de uma franquia . num_vendedores = [8, 20, 26, 2, 8, 20, 22, 12, 6, 16] num_de_vendas = [88, 169, 202, 58, 118, 157, 149, 117, 105, 137] . import matplotlib.pyplot as plt . plt.scatter(num_vendedores, num_de_vendas); . Como voc&#234; far&#225; no dia a dia . import numpy as np from sklearn.linear_model import LinearRegression . x = np.array(num_vendedores) y = np.array(num_de_vendas) . reg = LinearRegression().fit(x.reshape((-1,1)), y) . a, b = reg.coef_.item(), reg.intercept_ a, b . (5.000000000000001, 59.999999999999986) . Ou ainda usando apenas o numpy . a, b = np.polyfit(x, y, 1) a, b . (4.999999999999999, 60.000000000000014) . Visualizando a regress&#227;o linear . plt.scatter(num_vendedores, num_de_vendas); plt.axline((0, b), slope = a, color = &#39;red&#39;, linestyle = &#39;--&#39;); . Predi&#231;&#227;o (infer&#234;ncia) . nova_loja_num_vendedores = 14 nova_loja_venda_esperada = a * nova_loja_num_vendedores + b nova_loja_venda_esperada . 130.0 . plt.scatter(num_vendedores, num_de_vendas); plt.axline((0, b), slope = a, color = &#39;red&#39;, linestyle = &#39;--&#39;); plt.scatter(nova_loja_num_vendedores, nova_loja_venda_esperada, color = &#39;red&#39;); . Implementando (e entendendo) um modelo de regress&#227;o linear . import torch . x = torch.tensor(num_vendedores) y = torch.tensor(num_de_vendas) . 1 - Inicializa&#231;&#227;o . a = torch.randn(1).requires_grad_() b = torch.randn(1).requires_grad_() a, b . (tensor([-1.4139], requires_grad=True), tensor([1.0566], requires_grad=True)) . 2 - Predi&#231;&#227;o . y_pred = a * x + b y_pred . tensor([ -6.4832, -18.1523, -23.9869, -0.6486, -6.4832, -18.1523, -20.0972, -10.3729, -4.5383, -14.2626], grad_fn=&lt;AddBackward0&gt;) . plt.scatter(x, y); plt.scatter(x, y_pred.detach(), color = &#39;red&#39;); plt.axline((0, b.item()), slope = a.item(), color = &#39;red&#39;, linestyle = &#39;--&#39;); . 3 - Perda (erro) . erro = y - y_pred erro . tensor([ 94.4832, 187.1523, 225.9869, 58.6486, 124.4832, 175.1523, 169.0972, 127.3729, 109.5383, 151.2626], grad_fn=&lt;SubBackward0&gt;) . rmse = (erro**2).mean()**0.5 rmse . tensor(149.7778, grad_fn=&lt;PowBackward0&gt;) . 4 - Gradiente (&#129497; Mate-M&#225;gica!! &#10024;) . rmse.backward() . a.grad, b.grad . (tensor([-15.9815]), tensor([-1.1920])) . Step (passo) . a.data = a.data - a.grad b.data = b.data - b.grad . a.grad.zero_() b.grad.zero_(); . Repetir n-vezes . historico_rmse = [] ## Hiperparâmetros n_epoch = 10000 lr = 0.05 ## Inicialização a = torch.randn(1).requires_grad_() b = torch.randn(1).requires_grad_() ## Otimização for epoch in range(n_epoch): ## Predição y_pred = a * x + b ## Calcula perda (loss) erro = y - y_pred rmse = (erro**2).mean()**0.5 if epoch % 1000 == 0: print(f&#39;RMSE: {rmse.item()}&#39;) historico_rmse.append(rmse.item()) ## Calcula gradiente rmse.backward() ## Step a.data = a.data - lr * a.grad b.data = b.data - lr * b.grad a.grad.zero_() b.grad.zero_() . RMSE: 142.61758422851562 RMSE: 21.743059158325195 RMSE: 15.713376998901367 RMSE: 13.123400688171387 RMSE: 12.502219200134277 RMSE: 12.391326904296875 RMSE: 12.372918128967285 RMSE: 12.369906425476074 RMSE: 12.36941146850586 RMSE: 12.369332313537598 . plt.scatter(x, y); plt.axline((0, b.item()), slope = a.item(), color = &#39;red&#39;, linestyle = &#39;--&#39;); . a, b . (tensor([5.0009], requires_grad=True), tensor([59.9831], requires_grad=True)) . plt.plot(historico_rmse[:100]); .",
            "url": "https://opassos.github.io/blog/tutorial/ml/2021/10/23/Regressao_Linear.html",
            "relUrl": "/tutorial/ml/2021/10/23/Regressao_Linear.html",
            "date": " • Oct 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": ". &gt; Tutorial de classificação usando CNN - toc: true - comments: true - categories: [Tutorial, CNN] - image: images/NEU.png . Setup . !nvidia-smi . Thu Oct 28 20:38:57 2021 +--+ | NVIDIA-SMI 495.29.05 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 A100-SXM4-40GB Off | 00000000:00:04.0 Off | 0 | | N/A 49C P0 45W / 400W | 0MiB / 40536MiB | 0% Default | | | | Disabled | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . !pip install -Uqqq fastai . |████████████████████████████████| 189 kB 4.9 MB/s |████████████████████████████████| 56 kB 5.4 MB/s . Baixando os dados . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . !cp /content/drive/MyDrive/Datasets/NEU-CLS.rar NEU_CLS.rar . !unrar x NEU_CLS.rar &gt; /dev/null 2&gt;&amp;1 . !ls NEU-CLS . EDA . from fastai.vision.all import * . path = Path(&#39;NEU-CLS&#39;) . path.ls() . (#1801) [Path(&#39;NEU-CLS/Pa_80.bmp&#39;),Path(&#39;NEU-CLS/RS_202.bmp&#39;),Path(&#39;NEU-CLS/Pa_39.bmp&#39;),Path(&#39;NEU-CLS/Pa_17.bmp&#39;),Path(&#39;NEU-CLS/Sc_229.bmp&#39;),Path(&#39;NEU-CLS/Cr_254.bmp&#39;),Path(&#39;NEU-CLS/In_242.bmp&#39;),Path(&#39;NEU-CLS/Pa_202.bmp&#39;),Path(&#39;NEU-CLS/RS_14.bmp&#39;),Path(&#39;NEU-CLS/Sc_50.bmp&#39;)...] . img = path.ls()[1] img = Image.open(img) img . img.shape . (200, 200) . def labeler(o): return o.name.split(&#39;_&#39;)[0] labeler(path.ls()[0]) . &#39;Pa&#39; . files = get_image_files(path) files . (#1800) [Path(&#39;NEU-CLS/Pa_80.bmp&#39;),Path(&#39;NEU-CLS/RS_202.bmp&#39;),Path(&#39;NEU-CLS/Pa_39.bmp&#39;),Path(&#39;NEU-CLS/Pa_17.bmp&#39;),Path(&#39;NEU-CLS/Sc_229.bmp&#39;),Path(&#39;NEU-CLS/Cr_254.bmp&#39;),Path(&#39;NEU-CLS/In_242.bmp&#39;),Path(&#39;NEU-CLS/Pa_202.bmp&#39;),Path(&#39;NEU-CLS/RS_14.bmp&#39;),Path(&#39;NEU-CLS/Sc_50.bmp&#39;)...] . labels = L(map(labeler, files)) labels . (#1800) [&#39;Pa&#39;,&#39;RS&#39;,&#39;Pa&#39;,&#39;Pa&#39;,&#39;Sc&#39;,&#39;Cr&#39;,&#39;In&#39;,&#39;Pa&#39;,&#39;RS&#39;,&#39;Sc&#39;...] . labels.unique() . (#6) [&#39;Pa&#39;,&#39;RS&#39;,&#39;Sc&#39;,&#39;Cr&#39;,&#39;In&#39;,&#39;PS&#39;] . np.unique(np.array(labels), return_counts=True) . (array([&#39;Cr&#39;, &#39;In&#39;, &#39;PS&#39;, &#39;Pa&#39;, &#39;RS&#39;, &#39;Sc&#39;], dtype=&#39;&lt;U2&#39;), array([300, 300, 300, 300, 300, 300])) . Dataloader . dblock = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = labeler, # batch_tfms = aug_transforms(mult= 1.5, flip_vert=True, max_lighting=0.3), batch_tfms = [Dihedral(), Flip()], splitter = RandomSplitter(1/3, seed=42) ) dls = dblock.dataloaders(path, bs = 64, num_workers = 10) . dls.show_batch() . Fun&#231;&#227;o para plotar m&#233;tricas . @patch @delegates(subplots) def plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs): metrics = np.stack(self.values) names = self.metric_names[1:-1] n = len(names) - 1 if nrows is None and ncols is None: nrows = int(math.sqrt(n)) ncols = int(np.ceil(n / nrows)) elif nrows is None: nrows = int(np.ceil(n / ncols)) elif ncols is None: ncols = int(np.ceil(n / nrows)) figsize = figsize or (ncols * 6, nrows * 4) fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs) axs = [ax if i &lt; n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n] for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)): ax.plot(metrics[:, i], color=&#39;#1f77b4&#39; if i == 0 else &#39;#ff7f0e&#39;, label=&#39;valid&#39; if i &gt; 0 else &#39;train&#39;) ax.set_title(name if i &gt; 1 else &#39;losses&#39;) ax.legend(loc=&#39;best&#39;) plt.show() . Baseline learner (Resnet34) . base_learner = cnn_learner(dls, resnet34, metrics = accuracy) . base_learner.fine_tune(10) . epoch train_loss valid_loss accuracy time . 0 | 1.370985 | 0.233006 | 0.936667 | 00:02 | . epoch train_loss valid_loss accuracy time . 0 | 0.149249 | 0.117200 | 0.960000 | 00:02 | . 1 | 0.093157 | 0.033443 | 0.985000 | 00:02 | . 2 | 0.066716 | 0.011498 | 0.993333 | 00:02 | . 3 | 0.061090 | 0.008424 | 0.998333 | 00:02 | . 4 | 0.048788 | 0.013589 | 0.996667 | 00:02 | . 5 | 0.035898 | 0.002989 | 0.998333 | 00:02 | . 6 | 0.026968 | 0.004380 | 0.996667 | 00:02 | . 7 | 0.019932 | 0.002487 | 1.000000 | 00:02 | . 8 | 0.014018 | 0.003501 | 0.996667 | 00:02 | . 9 | 0.010632 | 0.004444 | 0.996667 | 00:02 | . base_learner.recorder.plot_metrics() . Squeezenet Learner . import torchvision.models as models . squeezenet = models.squeezenet1_0(pretrained=True) . fake_batch = torch.randn((2, 3, 200, 200)) plt.imshow(fake_batch[0].permute(1, 2, 0)); . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . squeezenet(fake_batch).shape . torch.Size([2, 1000]) . squeezenet.classifier[1] = nn.Conv2d(512, 6, kernel_size=(1, 1), stride=(1, 1)) . squeezenet(fake_batch).shape . torch.Size([2, 6]) . sqnt_learner = Learner(dls, squeezenet, metrics = accuracy) . sqnt_learner.lr_find() . SuggestedLRs(valley=5.248074739938602e-05) . sqnt_learner.fine_tune(10, 1e-4) . epoch train_loss valid_loss accuracy time . 0 | 1.666852 | 0.817575 | 0.815000 | 00:02 | . epoch train_loss valid_loss accuracy time . 0 | 0.731520 | 0.442989 | 0.895000 | 00:02 | . 1 | 0.505954 | 0.131973 | 0.955000 | 00:02 | . 2 | 0.319941 | 0.031790 | 0.990000 | 00:02 | . 3 | 0.214777 | 0.026260 | 0.993333 | 00:02 | . 4 | 0.149010 | 0.022425 | 0.993333 | 00:02 | . 5 | 0.105086 | 0.011306 | 0.995000 | 00:02 | . 6 | 0.076240 | 0.011093 | 0.998333 | 00:02 | . 7 | 0.057407 | 0.010996 | 0.995000 | 00:02 | . 8 | 0.041928 | 0.009450 | 0.998333 | 00:02 | . 9 | 0.032595 | 0.009261 | 0.998333 | 00:02 | . sqnt_learner.recorder.plot_metrics() . SqueezeNet vs. Resnet34 . sqnt_learner.summary() . SqueezeNet (Input shape: 64 x 3 x 200 x 200) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 96 x 97 x 97 Conv2d 14208 True ReLU ____________________________________________________________________________ 64 x 96 x 48 x 48 MaxPool2d ____________________________________________________________________________ 64 x 16 x 48 x 48 Conv2d 1552 True ReLU ____________________________________________________________________________ 64 x 64 x 48 x 48 Conv2d 1088 True ReLU Conv2d 9280 True ReLU ____________________________________________________________________________ 64 x 16 x 48 x 48 Conv2d 2064 True ReLU ____________________________________________________________________________ 64 x 64 x 48 x 48 Conv2d 1088 True ReLU Conv2d 9280 True ReLU ____________________________________________________________________________ 64 x 32 x 48 x 48 Conv2d 4128 True ReLU ____________________________________________________________________________ 64 x 128 x 48 x 48 Conv2d 4224 True ReLU Conv2d 36992 True ReLU ____________________________________________________________________________ 64 x 256 x 24 x 24 MaxPool2d ____________________________________________________________________________ 64 x 32 x 24 x 24 Conv2d 8224 True ReLU ____________________________________________________________________________ 64 x 128 x 24 x 24 Conv2d 4224 True ReLU Conv2d 36992 True ReLU ____________________________________________________________________________ 64 x 48 x 24 x 24 Conv2d 12336 True ReLU ____________________________________________________________________________ 64 x 192 x 24 x 24 Conv2d 9408 True ReLU Conv2d 83136 True ReLU ____________________________________________________________________________ 64 x 48 x 24 x 24 Conv2d 18480 True ReLU ____________________________________________________________________________ 64 x 192 x 24 x 24 Conv2d 9408 True ReLU Conv2d 83136 True ReLU ____________________________________________________________________________ 64 x 64 x 24 x 24 Conv2d 24640 True ReLU ____________________________________________________________________________ 64 x 256 x 24 x 24 Conv2d 16640 True ReLU Conv2d 147712 True ReLU ____________________________________________________________________________ 64 x 512 x 12 x 12 MaxPool2d ____________________________________________________________________________ 64 x 64 x 12 x 12 Conv2d 32832 True ReLU ____________________________________________________________________________ 64 x 256 x 12 x 12 Conv2d 16640 True ReLU Conv2d 147712 True ReLU Dropout ____________________________________________________________________________ 64 x 6 x 12 x 12 Conv2d 3078 True ReLU ____________________________________________________________________________ 64 x 6 x 1 x 1 AdaptiveAvgPool2d ____________________________________________________________________________ Total params: 738,502 Total trainable params: 738,502 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7eff01058050&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . base_learner.summary() . Sequential (Input shape: 64 x 3 x 200 x 200) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 64 x 100 x 100 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 64 x 64 x 50 x 50 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 64 x 128 x 25 x 25 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 64 x 256 x 13 x 13 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 64 x 512 x 7 x 7 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ 64 x 512 x 1 x 1 AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 64 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 64 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 64 x 6 Linear 3072 True ____________________________________________________________________________ Total params: 21,815,104 Total trainable params: 21,815,104 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7eff01058050&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Squeezenet (artigo) . dblock = DataBlock( blocks = (ImageBlock(cls = PILImageBW), CategoryBlock), get_items = get_image_files, get_y = labeler, # batch_tfms = aug_transforms(mult= 1.5, flip_vert=True, max_lighting=0.3), batch_tfms = [Dihedral(), Flip()], splitter = RandomSplitter(1/3, seed=42) ) dls = dblock.dataloaders(path, bs = 64, num_workers = 10) . class Fire(nn.Module): def __init__(self, inplanes: int, squeeze_planes: int, expand1x1_planes: int, expand3x3_planes: int) -&gt; None: super(Fire, self).__init__() self.inplanes = inplanes self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1) self.squeeze_activation = nn.ReLU(inplace=True) self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1) self.expand1x1_activation = nn.ReLU(inplace=True) self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1) self.expand3x3_activation = nn.ReLU(inplace=True) def forward(self, x: torch.Tensor) -&gt; torch.Tensor: x = self.squeeze_activation(self.squeeze(x)) return torch.cat( [self.expand1x1_activation(self.expand1x1(x)), self.expand3x3_activation(self.expand3x3(x))], 1 ) . num_classes = 6 n_ch = 1 squeezenet_passos = nn.Sequential( nn.Conv2d(n_ch, 32, kernel_size=7, stride=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(32, 4, 16, 16), Fire(32, 4, 16, 16), Fire(32, 8, 32, 32), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(64, 8, 32, 32), Fire(64, 12, 48, 48), Fire(96, 12, 48, 48), Fire(96, 16, 64, 64), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(128, 16, 64, 64), nn.Dropout(p=0.5), nn.Conv2d(128, num_classes, kernel_size=1), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten() ) for m in squeezenet_passos.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) nn.init.normal_(squeezenet_passos[-4].weight, mean=0.0, std=0.01) ; . squeezenet_passos_learner = Learner(dls, squeezenet_passos, metrics = accuracy) . . squeezenet_passos_learner.lr_find() . SuggestedLRs(valley=0.00013182566908653826) . squeezenet_passos_learner.fit_one_cycle(50, 1e-4) . epoch train_loss valid_loss accuracy time . 0 | 1.791900 | 1.791988 | 0.150000 | 00:02 | . 1 | 1.791775 | 1.791582 | 0.150000 | 00:02 | . 2 | 1.791623 | 1.790936 | 0.150000 | 00:02 | . 3 | 1.791279 | 1.789704 | 0.183333 | 00:02 | . 4 | 1.790509 | 1.785844 | 0.183333 | 00:02 | . 5 | 1.788433 | 1.772129 | 0.186111 | 00:02 | . 6 | 1.781431 | 1.741034 | 0.186111 | 00:02 | . 7 | 1.771553 | 1.720086 | 0.191667 | 00:02 | . 8 | 1.753636 | 1.659074 | 0.191667 | 00:02 | . 9 | 1.718735 | 1.582751 | 0.200000 | 00:02 | . 10 | 1.677779 | 1.525723 | 0.227778 | 00:02 | . 11 | 1.620832 | 1.391023 | 0.377778 | 00:02 | . 12 | 1.546230 | 1.308695 | 0.422222 | 00:02 | . 13 | 1.451604 | 1.106395 | 0.475000 | 00:02 | . 14 | 1.346076 | 0.948779 | 0.536111 | 00:02 | . 15 | 1.244202 | 0.922144 | 0.527778 | 00:02 | . 16 | 1.151950 | 0.855132 | 0.597222 | 00:02 | . 17 | 1.064838 | 0.804390 | 0.641667 | 00:02 | . 18 | 0.997430 | 0.752876 | 0.691667 | 00:02 | . 19 | 0.932847 | 0.759131 | 0.722222 | 00:02 | . 20 | 0.860047 | 0.629554 | 0.758333 | 00:02 | . 21 | 0.798997 | 0.603449 | 0.813889 | 00:02 | . 22 | 0.740198 | 0.601435 | 0.788889 | 00:02 | . 23 | 0.682720 | 0.599085 | 0.816667 | 00:02 | . 24 | 0.640688 | 0.479338 | 0.886111 | 00:02 | . 25 | 0.604215 | 0.572125 | 0.819444 | 00:02 | . 26 | 0.571876 | 0.435102 | 0.869444 | 00:02 | . 27 | 0.539309 | 0.413147 | 0.883333 | 00:02 | . 28 | 0.510766 | 0.478334 | 0.858333 | 00:02 | . 29 | 0.489436 | 0.441942 | 0.852778 | 00:02 | . 30 | 0.470699 | 0.387899 | 0.888889 | 00:02 | . 31 | 0.458267 | 0.419520 | 0.866667 | 00:02 | . 32 | 0.432825 | 0.415547 | 0.863889 | 00:02 | . 33 | 0.419693 | 0.418507 | 0.866667 | 00:02 | . 34 | 0.413952 | 0.358464 | 0.883333 | 00:02 | . 35 | 0.399802 | 0.381817 | 0.886111 | 00:02 | . 36 | 0.408416 | 0.348725 | 0.888889 | 00:02 | . 37 | 0.395493 | 0.333479 | 0.897222 | 00:02 | . 38 | 0.387950 | 0.380797 | 0.875000 | 00:02 | . 39 | 0.382390 | 0.393246 | 0.877778 | 00:02 | . 40 | 0.382642 | 0.376822 | 0.880556 | 00:02 | . 41 | 0.368031 | 0.339786 | 0.894444 | 00:02 | . 42 | 0.360568 | 0.407338 | 0.872222 | 00:02 | . 43 | 0.355441 | 0.343555 | 0.891667 | 00:02 | . 44 | 0.357080 | 0.371184 | 0.880556 | 00:02 | . 45 | 0.351340 | 0.353385 | 0.894444 | 00:02 | . 46 | 0.355300 | 0.364003 | 0.891667 | 00:02 | . 47 | 0.352543 | 0.368314 | 0.894444 | 00:02 | . 48 | 0.345402 | 0.367053 | 0.894444 | 00:02 | . 49 | 0.349676 | 0.366572 | 0.894444 | 00:02 | . squeezenet_passos_learner.recorder.plot_metrics() . ResSqueezeNet (artigo) . class ResSqueezeNetPassos(nn.Module): def __init__(self, num_classes: int = 6, dropout: float = 0.5) -&gt; None: super().__init__() self.num_classes = num_classes self.conv1 = nn.Conv2d(1, 32, kernel_size=7, stride=2) self.mp1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True) self.f1 = Fire(32, 4, 16, 16) self.f2 = Fire(32, 4, 16, 16) self.f3 = Fire(32, 8, 32, 32) self.mp2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True) self.f4 = Fire(64, 8, 32, 32) self.f5 = Fire(64, 12, 48, 48) self.f6 = Fire(96, 12, 48, 48) self.f7 = Fire(96, 16, 64, 64) self.mp3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True) self.f8 = Fire(128, 16, 64, 64) final_conv = nn.Conv2d(128, self.num_classes, kernel_size=1) self.classifier = nn.Sequential( nn.Dropout(p=dropout), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)) ) for m in self.modules(): if isinstance(m, nn.Conv2d): if m is final_conv: nn.init.normal_(m.weight, mean=0.0, std=0.01) else: nn.init.kaiming_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) def forward(self, x: torch.Tensor) -&gt; torch.Tensor: x = nn.Sequential( self.conv1, nn.ReLU(inplace=True), self.mp1, self.f1 )(x) x = self.f2(x) + x x = nn.Sequential( self.f3, self.mp2 )(x) x = self.f4(x) + x x = self.f5(x) x = self.f6(x) + x x = nn.Sequential( self.f7, self.mp3 )(x) x = self.f8(x) + x x = self.classifier(x) return torch.flatten(x, 1) . ressqueezenet_passos_learner = Learner(dls, ResSqueezeNetPassos(), metrics = accuracy) . ressqueezenet_passos_learner.fit_one_cycle(100, 3e-4) . epoch train_loss valid_loss accuracy time . 0 | 1.803072 | 1.807198 | 0.156667 | 00:02 | . 1 | 1.798336 | 1.792640 | 0.156667 | 00:01 | . 2 | 1.793250 | 1.778558 | 0.156667 | 00:01 | . 3 | 1.788431 | 1.765367 | 0.156667 | 00:01 | . 4 | 1.782662 | 1.755790 | 0.156667 | 00:01 | . 5 | 1.777332 | 1.750165 | 0.271667 | 00:01 | . 6 | 1.772359 | 1.742972 | 0.286667 | 00:01 | . 7 | 1.765739 | 1.726264 | 0.390000 | 00:01 | . 8 | 1.755395 | 1.691020 | 0.453333 | 00:01 | . 9 | 1.732128 | 1.613554 | 0.393333 | 00:01 | . 10 | 1.688152 | 1.464911 | 0.343333 | 00:01 | . 11 | 1.605122 | 1.247171 | 0.466667 | 00:01 | . 12 | 1.472738 | 1.025227 | 0.613333 | 00:01 | . 13 | 1.308919 | 0.796920 | 0.746667 | 00:01 | . 14 | 1.149675 | 0.733332 | 0.725000 | 00:01 | . 15 | 1.017071 | 0.600268 | 0.758333 | 00:01 | . 16 | 0.904640 | 0.526078 | 0.820000 | 00:01 | . 17 | 0.805589 | 0.536634 | 0.828333 | 00:01 | . 18 | 0.729077 | 0.443321 | 0.823333 | 00:01 | . 19 | 0.687559 | 0.519358 | 0.741667 | 00:01 | . 20 | 0.636512 | 0.505365 | 0.823333 | 00:01 | . 21 | 0.605051 | 0.433961 | 0.833333 | 00:01 | . 22 | 0.560536 | 0.344748 | 0.890000 | 00:01 | . 23 | 0.523100 | 0.377201 | 0.875000 | 00:01 | . 24 | 0.487854 | 0.348990 | 0.898333 | 00:01 | . 25 | 0.463896 | 0.312746 | 0.903333 | 00:01 | . 26 | 0.449684 | 0.350009 | 0.895000 | 00:01 | . 27 | 0.460322 | 0.313103 | 0.890000 | 00:01 | . 28 | 0.446005 | 0.325945 | 0.906667 | 00:01 | . 29 | 0.410402 | 0.288701 | 0.898333 | 00:01 | . 30 | 0.391862 | 0.343657 | 0.886667 | 00:01 | . 31 | 0.371900 | 0.351702 | 0.891667 | 00:01 | . 32 | 0.371173 | 0.347791 | 0.883333 | 00:01 | . 33 | 0.382457 | 0.363712 | 0.875000 | 00:01 | . 34 | 0.363582 | 0.293444 | 0.906667 | 00:01 | . 35 | 0.348279 | 0.262752 | 0.915000 | 00:01 | . 36 | 0.321681 | 0.253379 | 0.930000 | 00:01 | . 37 | 0.308520 | 0.251440 | 0.933333 | 00:01 | . 38 | 0.311584 | 0.280496 | 0.905000 | 00:01 | . 39 | 0.296189 | 0.232784 | 0.928333 | 00:01 | . 40 | 0.289960 | 0.256331 | 0.916667 | 00:01 | . 41 | 0.285229 | 0.262350 | 0.928333 | 00:01 | . 42 | 0.290730 | 0.345040 | 0.881667 | 00:01 | . 43 | 0.287286 | 0.215360 | 0.931667 | 00:01 | . 44 | 0.270833 | 0.204593 | 0.935000 | 00:01 | . 45 | 0.257251 | 0.192188 | 0.941667 | 00:01 | . 46 | 0.247839 | 0.223318 | 0.936667 | 00:02 | . 47 | 0.235811 | 0.196975 | 0.935000 | 00:01 | . 48 | 0.230695 | 0.259695 | 0.908333 | 00:01 | . 49 | 0.229419 | 0.194092 | 0.928333 | 00:01 | . 50 | 0.228610 | 0.269237 | 0.908333 | 00:01 | . 51 | 0.229168 | 0.172121 | 0.951667 | 00:01 | . 52 | 0.221312 | 0.176350 | 0.948333 | 00:01 | . 53 | 0.220383 | 0.269990 | 0.896667 | 00:01 | . 54 | 0.222094 | 0.247141 | 0.925000 | 00:01 | . 55 | 0.223976 | 0.269836 | 0.905000 | 00:01 | . 56 | 0.214664 | 0.178309 | 0.946667 | 00:01 | . 57 | 0.205079 | 0.202422 | 0.935000 | 00:01 | . 58 | 0.199331 | 0.168464 | 0.948333 | 00:01 | . 59 | 0.200643 | 0.250318 | 0.910000 | 00:01 | . 60 | 0.204680 | 0.176340 | 0.951667 | 00:01 | . 61 | 0.199009 | 0.161006 | 0.950000 | 00:01 | . 62 | 0.194183 | 0.288742 | 0.916667 | 00:01 | . 63 | 0.194596 | 0.174039 | 0.940000 | 00:01 | . 64 | 0.197789 | 0.189094 | 0.936667 | 00:01 | . 65 | 0.188060 | 0.177251 | 0.950000 | 00:01 | . 66 | 0.186816 | 0.173901 | 0.946667 | 00:01 | . 67 | 0.183516 | 0.173670 | 0.940000 | 00:01 | . 68 | 0.179931 | 0.197407 | 0.945000 | 00:01 | . 69 | 0.179673 | 0.177913 | 0.943333 | 00:01 | . 70 | 0.173193 | 0.142216 | 0.960000 | 00:01 | . 71 | 0.170666 | 0.176059 | 0.948333 | 00:01 | . 72 | 0.170354 | 0.138018 | 0.963333 | 00:01 | . 73 | 0.168197 | 0.144024 | 0.958333 | 00:01 | . 74 | 0.162371 | 0.148737 | 0.951667 | 00:01 | . 75 | 0.158917 | 0.142324 | 0.958333 | 00:01 | . 76 | 0.157264 | 0.180217 | 0.948333 | 00:01 | . 77 | 0.155334 | 0.146097 | 0.956667 | 00:01 | . 78 | 0.154176 | 0.136606 | 0.961667 | 00:01 | . 79 | 0.152948 | 0.145297 | 0.958333 | 00:01 | . 80 | 0.149063 | 0.134969 | 0.963333 | 00:02 | . 81 | 0.149257 | 0.144715 | 0.958333 | 00:01 | . 82 | 0.149718 | 0.150653 | 0.953333 | 00:01 | . 83 | 0.150473 | 0.169222 | 0.955000 | 00:01 | . 84 | 0.155219 | 0.144374 | 0.960000 | 00:01 | . 85 | 0.154493 | 0.132473 | 0.963333 | 00:01 | . 86 | 0.151574 | 0.133986 | 0.960000 | 00:01 | . 87 | 0.149089 | 0.134131 | 0.960000 | 00:01 | . 88 | 0.149381 | 0.134098 | 0.958333 | 00:01 | . 89 | 0.147147 | 0.138082 | 0.961667 | 00:01 | . 90 | 0.146902 | 0.134274 | 0.960000 | 00:01 | . 91 | 0.145153 | 0.135444 | 0.961667 | 00:01 | . 92 | 0.142515 | 0.133553 | 0.963333 | 00:01 | . 93 | 0.140086 | 0.134400 | 0.961667 | 00:01 | . 94 | 0.140473 | 0.134768 | 0.961667 | 00:02 | . 95 | 0.140447 | 0.133311 | 0.963333 | 00:02 | . 96 | 0.138581 | 0.132012 | 0.961667 | 00:01 | . 97 | 0.138237 | 0.131875 | 0.961667 | 00:01 | . 98 | 0.140701 | 0.132013 | 0.961667 | 00:01 | . 99 | 0.139985 | 0.132024 | 0.961667 | 00:01 | . ressqueezenet_passos_learner.recorder.plot_metrics() .",
            "url": "https://opassos.github.io/blog/2021/10/23/Defeito-em-Chapas.html",
            "relUrl": "/2021/10/23/Defeito-em-Chapas.html",
            "date": " • Oct 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Como Fazer uma EDA (competição PETS-Kaggle 🙀x🐶)",
            "content": ". Setup . Como usar a API do Kaggle no colab . !pip install -Uqqq kaggle !pip install -Uqqq fastai . |████████████████████████████████| 186 kB 16.1 MB/s |████████████████████████████████| 56 kB 4.0 MB/s . from google.colab import files uploaded = files.upload() uploaded.keys() !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . from kaggle.api.kaggle_api_extended import KaggleApi api = KaggleApi() api.authenticate() . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path . Baixando o dataset da competi&#231;&#227;o . api.competition_download_files(&#39;petfinder-pawpularity-score&#39;) . !unzip petfinder-pawpularity-score.zip -d comp_data &gt; /dev/null 2&gt;&amp;1 . Explorando o CSV . train_df = pd.read_csv(&#39;/content/comp_data/train.csv&#39;) train_df.head() . Id Subject Focus Eyes Face Near Action Accessory Group Collage Human Occlusion Info Blur Pawpularity . 0 0007de18844b0dbbb5e1f607da0606e0 | 0 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 63 | . 1 0009c66b9439883ba2750fb825e1d7db | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | . 2 0013fd999caf9a3efe1352ca1b0d937e | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 28 | . 3 0018df346ac9c1d8413cfcc888ca8246 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | . 4 001dc955e10590d3ca4673f034feeef2 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 72 | . train_df[&#39;Pawpularity&#39;].nunique() . 100 . train_df[&#39;Pawpularity&#39;].value_counts() . 28 318 30 318 26 316 31 312 29 304 ... 98 10 97 8 90 7 1 4 99 4 Name: Pawpularity, Length: 100, dtype: int64 . sns.set(rc={&#39;figure.figsize&#39;:(14,7)}) fig = plt.figure() sns.histplot(data=train_df, x=&#39;Pawpularity&#39;, kde=True) plt.title(&#39;Histograma do score &quot;Pawpularity&quot;&#39;, fontsize=20, fontweight=&#39;bold&#39;) plt.show() . sns.set(rc={&#39;figure.figsize&#39;:(21,15)}) predictor = train_df.columns[1:-1] fig = plt.figure() for i, p in enumerate(predictor): ax = plt.subplot(3,4,i+1) sns.countplot(data=train_df, x=p, ax=ax) ax.set_xlabel(None) ax.set_title(p, fontweight=&#39;bold&#39;, color=&quot;#e7273e&quot;, y=-0.1) plt.show() . sns.set(rc={&#39;figure.figsize&#39;:(12,10)}) predictor = train_df.columns[1:] corr_matrix = train_df[predictor].corr() fig = plt.figure() sns.set_theme(style=&quot;white&quot;) mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) cmap = sns.diverging_palette(230, 20, as_cmap=True) sns.heatmap(corr_matrix, annot=True, fmt=&#39;.1g&#39;, cmap=cmap, mask=mask, square=True) plt.title(&#39;Matriz de Correlação&#39;, fontsize=20, fontweight=&#39;bold&#39;) plt.show() . Explorando as fotos . train_path = Path(&#39;/content/comp_data/train&#39;) . train_df[&#39;path&#39;] = train_df[&#39;Id&#39;].apply(lambda x: train_path/(x + &#39;.jpg&#39;)) . plt.imshow(plt.imread(train_df.loc[0, &#39;path&#39;])); . Fotos por Pawpularity . def plot_pawpularity(score, n): imgs = train_df.query(f&#39;Pawpularity == {score}&#39;).sample(n)[&#39;path&#39;] for i, img in enumerate(imgs): img = plt.imread(img) plt.subplot(1, n, i+1) plt.title(score) plt.axis(&#39;off&#39;) plt.imshow(img) . plot_pawpularity(10, 4) . plot_pawpularity(30, 4) . plot_pawpularity(90, 4) . Baseline Simples . def RMSE(pred, targ): return (((pred - targ)**2).mean())**0.5 . sns.set(rc={&#39;figure.figsize&#39;:(8,5)}) . train_data = pd.read_csv(&#39;/content/comp_data/train.csv&#39;) . n_samp = len(train_data) np.random.seed(42) split = np.random.randint(0, n_samp, n_samp) split . array([7270, 860, 5390, ..., 4926, 9582, 9304]) . train = train_data[&#39;Pawpularity&#39;].values[split[:int(0.8*n_samp)]] test = train_data[&#39;Pawpularity&#39;].values[split[int(0.8*n_samp):]] . rmse = [] for n in range(10000): preds = np.random.choice(train, len(test), replace=False) rmse.append(RMSE(preds, test)) rmse = np.array(rmse) . plt.hist(rmse, bins = 30); . plt.hist(train) plt.hist(preds) plt.hist(test); . Baseline mais simples ainda . preds = train.mean() RMSE(test, preds) . 20.918360445610613 . preds = np.median(train) RMSE(test, preds) . 21.472413666461463 . from scipy.stats import mode preds = mode(train)[0].item() RMSE(test, preds) . 22.342630590521075 . Influencia de gato/cachorro . from fastai.vision.all import * . path = untar_data(URLs.PETS)/&#39;images&#39; def labeler(x): return &#39;Gato&#39; if x.name[0].isupper() else &#39;Cachorro&#39; dblock = DataBlock( blocks = [ImageBlock, CategoryBlock], splitter = RandomSplitter(valid_pct = 0.2, seed = 42), get_items = get_image_files, get_y = labeler, item_tfms = Resize(256), ) dls = dblock.dataloaders(path) learn = cnn_learner(dls, resnet50, metrics=accuracy) learn.fine_tune(5) . . 100.00% [811712512/811706944 00:25&lt;00:00] Downloading: &#34;https://download.pytorch.org/models/resnet50-0676ba61.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss accuracy time . 0 | 0.097311 | 0.009561 | 0.997294 | 00:27 | . epoch train_loss valid_loss accuracy time . 0 | 0.042957 | 0.043040 | 0.990528 | 00:31 | . 1 | 0.031512 | 0.006856 | 0.996617 | 00:31 | . 2 | 0.027372 | 0.008505 | 0.996617 | 00:31 | . 3 | 0.009047 | 0.003232 | 0.998647 | 00:31 | . 4 | 0.004652 | 0.003530 | 0.998647 | 00:31 | . Inferencia . dls = DataBlock( blocks = [ImageBlock], get_x = ColReader(&#39;path&#39;), item_tfms = Resize(256), ).dataloaders(train_df).test_dl(train_df) dls.show_batch() . preds, _ = learn.get_preds(dl = dls) . train_df[&#39;tipo&#39;] = [&#39;Cachorro&#39; if p &gt; 0.5 else &#39;Gato&#39; for p in preds[:,0]] train_df[&#39;tipo&#39;].value_counts() . Gato 5969 Cachorro 3943 Name: tipo, dtype: int64 . sns.set(rc={&#39;figure.figsize&#39;:(14,7)}) fig, ax = plt.subplots(1,2) sns.boxplot(data=train_df, x=&#39;tipo&#39;, y=&#39;Pawpularity&#39;, ax=ax[0]) sns.histplot(train_df, x=&quot;Pawpularity&quot;, hue=&quot;tipo&quot;, kde=True, ax=ax[1]) fig.show() . train_df.groupby(&#39;tipo&#39;)[&#39;Pawpularity&#39;].mean() . tipo Cachorro 41.903119 Gato 35.486514 Name: Pawpularity, dtype: float64 . train = train_df.loc[split[:int(0.8*n_samp)]] test = train_df.loc[split[int(0.8*n_samp):]] . m_cachorro, m_gato = train.groupby(&#39;tipo&#39;)[&#39;Pawpularity&#39;].mean().values . preds = test[&#39;tipo&#39;].apply(lambda x: m_cachorro if x == &#39;Cachorro&#39; else m_gato).values RMSE(test[&#39;Pawpularity&#39;], preds) . 20.68950148400463 .",
            "url": "https://opassos.github.io/blog/tutorial/eda/2021/10/22/EDA-PetFinder.html",
            "relUrl": "/tutorial/eda/2021/10/22/EDA-PetFinder.html",
            "date": " • Oct 22, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Como uma IA Enxerga o Mundo? 🤖",
            "content": ". Setup . !nvidia-smi . Sat Oct 16 21:51:52 2021 +--+ | NVIDIA-SMI 470.74 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 A100-SXM4-40GB Off | 00000000:00:04.0 Off | 0 | | N/A 32C P0 43W / 400W | 0MiB / 40536MiB | 0% Default | | | | Disabled | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . !pip install -Uqqq fastai . |████████████████████████████████| 186 kB 5.1 MB/s |████████████████████████████████| 56 kB 4.8 MB/s . from fastai.vision.all import * . Dataset (MNIST) . path = untar_data(URLs.MNIST) . . 100.03% [15687680/15683414 00:00&lt;00:00] (path/&#39;testing&#39;).ls() . (#10) [Path(&#39;/root/.fastai/data/mnist_png/testing/3&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/9&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/2&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/5&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/7&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/8&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/1&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/4&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/0&#39;)] . (path/&#39;testing/6&#39;).ls() . (#958) [Path(&#39;/root/.fastai/data/mnist_png/testing/6/6933.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/3744.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/4239.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/3657.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/9138.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/366.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/8341.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/2728.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/3121.png&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing/6/2170.png&#39;)...] . um_seis = Image.open((path/&#39;testing/6&#39;).ls()[0]) um_seis . um_seis = tensor(um_seis) um_seis.shape . torch.Size([28, 28]) . um_seis[4:14, 4:14] . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 70, 252], [ 0, 0, 0, 0, 0, 0, 0, 0, 112, 252], [ 0, 0, 0, 0, 0, 0, 0, 95, 246, 252], [ 0, 0, 0, 0, 0, 0, 3, 170, 253, 253], [ 0, 0, 0, 0, 0, 0, 118, 252, 252, 214], [ 0, 0, 0, 0, 0, 85, 253, 252, 233, 33], [ 0, 0, 0, 0, 0, 157, 253, 252, 89, 0], [ 0, 0, 0, 0, 0, 230, 253, 252, 69, 0], [ 0, 0, 0, 0, 51, 243, 255, 249, 63, 0], [ 0, 0, 0, 0, 93, 252, 253, 132, 0, 0]], dtype=torch.uint8) . 2**8 - 1 . 255 . EDA . (pd.DataFrame(um_seis) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 255, vmin = 0) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 104 | 253 | 181 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 215 | 252 | 249 | 75 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 70 | 252 | 252 | 199 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 112 | 252 | 252 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 95 | 246 | 252 | 252 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 170 | 253 | 253 | 128 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 118 | 252 | 252 | 214 | 18 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 253 | 252 | 233 | 33 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 157 | 253 | 252 | 89 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 230 | 253 | 252 | 69 | 0 | 0 | 0 | 0 | 0 | 95 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 51 | 243 | 255 | 249 | 63 | 0 | 0 | 0 | 36 | 222 | 253 | 253 | 181 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 93 | 252 | 253 | 132 | 0 | 0 | 0 | 89 | 219 | 252 | 252 | 252 | 253 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 93 | 252 | 253 | 92 | 0 | 0 | 32 | 222 | 252 | 252 | 195 | 246 | 253 | 240 | 50 | 0 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 187 | 252 | 253 | 92 | 0 | 0 | 210 | 253 | 252 | 153 | 9 | 230 | 253 | 252 | 69 | 0 | 0 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 64 | 248 | 252 | 232 | 8 | 0 | 189 | 250 | 253 | 106 | 38 | 210 | 250 | 253 | 157 | 6 | 0 | 0 | 0 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 208 | 253 | 233 | 9 | 81 | 253 | 253 | 221 | 5 | 138 | 253 | 253 | 242 | 42 | 0 | 0 | 0 | 0 | 0 | 0 | . 18 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 169 | 227 | 253 | 173 | 197 | 252 | 252 | 193 | 136 | 252 | 252 | 252 | 178 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 253 | 252 | 252 | 252 | 252 | 253 | 252 | 252 | 252 | 221 | 63 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 56 | 219 | 252 | 252 | 252 | 253 | 252 | 252 | 218 | 88 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 54 | 137 | 242 | 253 | 178 | 137 | 35 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 27 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . um_seis = um_seis/255 . (pd.DataFrame(um_seis) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = 0) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.41 | 0.99 | 0.71 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.84 | 0.99 | 0.98 | 0.29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.27 | 0.99 | 0.99 | 0.78 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.44 | 0.99 | 0.99 | 0.45 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.37 | 0.96 | 0.99 | 0.99 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.67 | 0.99 | 0.99 | 0.50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.46 | 0.99 | 0.99 | 0.84 | 0.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.33 | 0.99 | 0.99 | 0.91 | 0.13 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.62 | 0.99 | 0.99 | 0.35 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.90 | 0.99 | 0.99 | 0.27 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.37 | 0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.20 | 0.95 | 1.00 | 0.98 | 0.25 | 0.00 | 0.00 | 0.00 | 0.14 | 0.87 | 0.99 | 0.99 | 0.71 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.36 | 0.99 | 0.99 | 0.52 | 0.00 | 0.00 | 0.00 | 0.35 | 0.86 | 0.99 | 0.99 | 0.99 | 0.99 | 0.64 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.36 | 0.99 | 0.99 | 0.36 | 0.00 | 0.00 | 0.13 | 0.87 | 0.99 | 0.99 | 0.76 | 0.96 | 0.99 | 0.94 | 0.20 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.73 | 0.99 | 0.99 | 0.36 | 0.00 | 0.00 | 0.82 | 0.99 | 0.99 | 0.60 | 0.04 | 0.90 | 0.99 | 0.99 | 0.27 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.25 | 0.97 | 0.99 | 0.91 | 0.03 | 0.00 | 0.74 | 0.98 | 0.99 | 0.42 | 0.15 | 0.82 | 0.98 | 0.99 | 0.62 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.82 | 0.99 | 0.91 | 0.04 | 0.32 | 0.99 | 0.99 | 0.87 | 0.02 | 0.54 | 0.99 | 0.99 | 0.95 | 0.16 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.66 | 0.89 | 0.99 | 0.68 | 0.77 | 0.99 | 0.99 | 0.76 | 0.53 | 0.99 | 0.99 | 0.99 | 0.70 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.13 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.87 | 0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.22 | 0.86 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.85 | 0.35 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 | 0.21 | 0.54 | 0.95 | 0.99 | 0.70 | 0.54 | 0.14 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . Blur . kernel = torch.ones((3,3))/9 kernel . tensor([[0.1111, 0.1111, 0.1111], [0.1111, 0.1111, 0.1111], [0.1111, 0.1111, 0.1111]]) . um_seis.view(1,1,28,28).shape . torch.Size([1, 1, 28, 28]) . um_seis_borrado = F.conv2d(um_seis.view(1,1,28,28), kernel.view(1,1,3,3), padding=1) . (pd.DataFrame(um_seis_borrado.view(28,28)) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = 0) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.16 | 0.23 | 0.19 | 0.08 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.14 | 0.36 | 0.55 | 0.44 | 0.22 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.04 | 0.28 | 0.61 | 0.85 | 0.64 | 0.31 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.08 | 0.40 | 0.73 | 0.89 | 0.61 | 0.28 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.04 | 0.23 | 0.56 | 0.85 | 0.80 | 0.47 | 0.14 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 | 0.38 | 0.71 | 0.87 | 0.66 | 0.33 | 0.06 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.28 | 0.61 | 0.87 | 0.81 | 0.49 | 0.18 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.04 | 0.20 | 0.49 | 0.78 | 0.83 | 0.60 | 0.28 | 0.06 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.11 | 0.38 | 0.71 | 0.85 | 0.69 | 0.37 | 0.12 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.21 | 0.54 | 0.87 | 0.83 | 0.51 | 0.18 | 0.01 | 0.00 | 0.00 | 0.04 | 0.07 | 0.07 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.30 | 0.63 | 0.93 | 0.76 | 0.42 | 0.10 | 0.00 | 0.02 | 0.11 | 0.26 | 0.39 | 0.37 | 0.22 | 0.08 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 | 0.38 | 0.71 | 0.92 | 0.66 | 0.33 | 0.06 | 0.04 | 0.15 | 0.36 | 0.58 | 0.72 | 0.70 | 0.51 | 0.26 | 0.08 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.10 | 0.43 | 0.76 | 0.86 | 0.57 | 0.23 | 0.04 | 0.15 | 0.37 | 0.67 | 0.84 | 0.95 | 0.93 | 0.81 | 0.50 | 0.20 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.16 | 0.49 | 0.82 | 0.80 | 0.47 | 0.14 | 0.11 | 0.35 | 0.67 | 0.85 | 0.80 | 0.80 | 0.85 | 0.93 | 0.67 | 0.34 | 0.05 | 0.00 | 0.00 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.26 | 0.59 | 0.88 | 0.73 | 0.41 | 0.17 | 0.30 | 0.61 | 0.80 | 0.78 | 0.64 | 0.69 | 0.83 | 0.93 | 0.67 | 0.34 | 0.05 | 0.00 | 0.00 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.31 | 0.64 | 0.92 | 0.69 | 0.40 | 0.28 | 0.54 | 0.82 | 0.79 | 0.62 | 0.51 | 0.67 | 0.85 | 0.84 | 0.56 | 0.23 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.30 | 0.62 | 0.90 | 0.71 | 0.52 | 0.51 | 0.75 | 0.92 | 0.73 | 0.58 | 0.61 | 0.83 | 0.93 | 0.71 | 0.38 | 0.09 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.16 | 0.39 | 0.71 | 0.73 | 0.74 | 0.75 | 0.89 | 0.95 | 0.79 | 0.74 | 0.78 | 0.93 | 0.86 | 0.55 | 0.23 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.07 | 0.19 | 0.43 | 0.64 | 0.83 | 0.92 | 0.96 | 0.96 | 0.91 | 0.91 | 0.92 | 0.89 | 0.66 | 0.35 | 0.11 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.15 | 0.36 | 0.59 | 0.73 | 0.85 | 0.94 | 0.95 | 0.91 | 0.80 | 0.63 | 0.38 | 0.16 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.13 | 0.26 | 0.40 | 0.52 | 0.61 | 0.62 | 0.58 | 0.47 | 0.32 | 0.15 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.03 | 0.09 | 0.19 | 0.28 | 0.29 | 0.25 | 0.15 | 0.07 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . Muito borrada . kernel = torch.ones((7,7)) kernel = kernel/kernel.sum() kernel . tensor([[0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204], [0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204], [0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204], [0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204], [0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204], [0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204], [0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204, 0.0204]]) . Dica: para manter a imagem do mesmo tamanho, padding vale k//2 . um_seis_muito_borrado = F.conv2d( um_seis.view(1,1,28,28), kernel.view(1,1,7,7), padding=3) . (pd.DataFrame(um_seis_muito_borrado.view(28,28)) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = 0) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.07 | 0.10 | 0.11 | 0.11 | 0.11 | 0.11 | 0.08 | 0.04 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.05 | 0.11 | 0.16 | 0.17 | 0.17 | 0.17 | 0.16 | 0.12 | 0.06 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.08 | 0.16 | 0.22 | 0.23 | 0.23 | 0.23 | 0.21 | 0.15 | 0.07 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.04 | 0.13 | 0.23 | 0.29 | 0.30 | 0.30 | 0.29 | 0.25 | 0.17 | 0.07 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.08 | 0.18 | 0.29 | 0.35 | 0.36 | 0.36 | 0.34 | 0.28 | 0.18 | 0.07 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.05 | 0.13 | 0.25 | 0.36 | 0.42 | 0.43 | 0.42 | 0.38 | 0.30 | 0.18 | 0.07 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.04 | 0.10 | 0.19 | 0.31 | 0.40 | 0.45 | 0.45 | 0.42 | 0.36 | 0.26 | 0.14 | 0.05 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.07 | 0.15 | 0.25 | 0.35 | 0.42 | 0.45 | 0.43 | 0.38 | 0.30 | 0.20 | 0.10 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.04 | 0.11 | 0.21 | 0.31 | 0.39 | 0.44 | 0.45 | 0.42 | 0.34 | 0.25 | 0.16 | 0.07 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 | 0.15 | 0.27 | 0.37 | 0.43 | 0.46 | 0.46 | 0.40 | 0.33 | 0.24 | 0.17 | 0.12 | 0.09 | 0.09 | 0.09 | 0.07 | 0.04 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.09 | 0.20 | 0.32 | 0.40 | 0.44 | 0.45 | 0.45 | 0.39 | 0.32 | 0.24 | 0.21 | 0.21 | 0.21 | 0.20 | 0.18 | 0.14 | 0.09 | 0.05 | 0.01 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.12 | 0.25 | 0.37 | 0.42 | 0.44 | 0.45 | 0.45 | 0.40 | 0.32 | 0.27 | 0.28 | 0.31 | 0.34 | 0.32 | 0.28 | 0.22 | 0.16 | 0.09 | 0.04 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.15 | 0.29 | 0.40 | 0.44 | 0.44 | 0.46 | 0.47 | 0.41 | 0.34 | 0.30 | 0.35 | 0.42 | 0.45 | 0.42 | 0.36 | 0.29 | 0.22 | 0.14 | 0.06 | 0.01 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.06 | 0.19 | 0.33 | 0.42 | 0.43 | 0.45 | 0.48 | 0.50 | 0.43 | 0.37 | 0.36 | 0.45 | 0.53 | 0.55 | 0.50 | 0.43 | 0.36 | 0.27 | 0.17 | 0.08 | 0.01 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.08 | 0.21 | 0.35 | 0.42 | 0.44 | 0.47 | 0.53 | 0.54 | 0.47 | 0.42 | 0.45 | 0.56 | 0.64 | 0.65 | 0.57 | 0.50 | 0.42 | 0.32 | 0.19 | 0.08 | 0.01 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.09 | 0.23 | 0.37 | 0.43 | 0.45 | 0.51 | 0.58 | 0.60 | 0.54 | 0.51 | 0.56 | 0.67 | 0.75 | 0.73 | 0.65 | 0.57 | 0.46 | 0.35 | 0.21 | 0.08 | 0.01 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.08 | 0.21 | 0.35 | 0.41 | 0.45 | 0.52 | 0.62 | 0.66 | 0.63 | 0.60 | 0.66 | 0.75 | 0.79 | 0.76 | 0.65 | 0.56 | 0.45 | 0.33 | 0.20 | 0.08 | 0.01 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.08 | 0.18 | 0.30 | 0.37 | 0.43 | 0.53 | 0.64 | 0.70 | 0.70 | 0.69 | 0.73 | 0.79 | 0.79 | 0.73 | 0.61 | 0.51 | 0.40 | 0.29 | 0.16 | 0.07 | 0.01 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.07 | 0.15 | 0.25 | 0.32 | 0.38 | 0.49 | 0.62 | 0.69 | 0.70 | 0.70 | 0.74 | 0.77 | 0.74 | 0.64 | 0.52 | 0.42 | 0.32 | 0.22 | 0.12 | 0.04 | 0.01 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.06 | 0.12 | 0.20 | 0.25 | 0.32 | 0.43 | 0.54 | 0.61 | 0.62 | 0.62 | 0.67 | 0.68 | 0.64 | 0.53 | 0.42 | 0.34 | 0.26 | 0.16 | 0.08 | 0.02 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.07 | 0.13 | 0.19 | 0.26 | 0.35 | 0.45 | 0.51 | 0.54 | 0.55 | 0.58 | 0.58 | 0.53 | 0.43 | 0.34 | 0.27 | 0.19 | 0.11 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.03 | 0.08 | 0.13 | 0.19 | 0.26 | 0.34 | 0.41 | 0.45 | 0.48 | 0.49 | 0.47 | 0.42 | 0.34 | 0.26 | 0.20 | 0.12 | 0.06 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.07 | 0.11 | 0.16 | 0.22 | 0.28 | 0.33 | 0.36 | 0.36 | 0.34 | 0.30 | 0.24 | 0.18 | 0.12 | 0.07 | 0.03 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.05 | 0.08 | 0.12 | 0.16 | 0.19 | 0.22 | 0.22 | 0.20 | 0.17 | 0.13 | 0.09 | 0.06 | 0.03 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.04 | 0.06 | 0.07 | 0.08 | 0.08 | 0.08 | 0.07 | 0.05 | 0.03 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . &#39;desborrado&#39; . kernel = torch.tensor( [[ 0, -1, 0], [-1, 5, -1], [ 0, -1, 0]], dtype = torch.float ) um_seis_desborrado = F.conv2d(um_seis.view(1,1,28,28), kernel.view(1,1,3,3), padding=1) (pd.DataFrame(um_seis_desborrado.view(28,28)) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = 0) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.41 | -0.99 | -0.71 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.46 | 0.20 | 2.85 | 1.55 | -0.83 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.05 | -0.86 | 1.78 | 1.14 | 2.11 | 0.46 | -0.29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.27 | -0.11 | 1.85 | 1.20 | 1.48 | -1.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.81 | -0.03 | 1.54 | 1.52 | 0.46 | -0.45 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.38 | 0.23 | 2.03 | 1.01 | 2.42 | -1.23 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.01 | -1.07 | 0.97 | 1.35 | 1.64 | 0.46 | -0.55 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.80 | 0.32 | 1.84 | 1.21 | 2.02 | -0.99 | -0.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.33 | 0.06 | 2.18 | 1.06 | 2.11 | -1.11 | -0.20 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.62 | 0.85 | 1.37 | 1.62 | -0.43 | -0.48 | 0.00 | 0.00 | 0.00 | 0.00 | -0.37 | -0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -1.10 | 1.95 | 1.08 | 1.71 | -0.23 | -0.27 | 0.00 | 0.00 | -0.14 | -1.24 | 0.62 | -0.13 | -0.96 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.20 | -0.32 | 1.67 | 1.09 | 2.13 | -0.01 | -0.25 | 0.00 | -0.49 | -1.02 | 2.23 | 1.74 | 2.02 | 1.53 | -1.18 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.36 | 0.27 | 1.64 | 1.46 | 0.26 | -0.76 | 0.00 | -0.47 | 0.02 | 1.83 | 1.24 | 1.21 | 1.00 | 1.63 | 1.25 | -0.84 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.36 | -0.26 | 1.61 | 1.63 | -0.07 | -0.36 | -0.13 | -1.07 | 1.90 | 1.24 | 1.60 | 0.85 | 1.18 | 1.07 | 1.89 | -0.23 | -0.20 | 0.00 | 0.00 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.98 | 1.34 | 1.24 | 1.71 | 0.42 | -0.36 | -1.56 | 2.02 | 1.29 | 1.95 | 0.84 | -2.91 | 1.54 | 1.09 | 2.12 | 0.15 | -0.27 | 0.00 | 0.00 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.25 | 0.28 | 2.07 | 1.08 | 1.62 | -1.15 | -1.09 | 1.73 | 1.35 | 1.71 | -0.07 | -1.64 | 1.96 | 1.19 | 1.42 | 0.91 | -0.77 | -0.02 | 0.00 | 0.00 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -1.07 | 1.45 | 1.35 | 1.64 | -1.76 | -0.21 | 1.92 | 1.13 | 1.57 | -2.26 | 0.56 | 1.62 | 1.05 | 1.90 | -0.74 | -0.19 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.66 | 1.61 | 1.68 | 1.49 | 0.60 | 0.89 | 1.20 | 1.22 | 0.40 | -0.09 | 1.89 | 0.98 | 1.40 | 1.31 | -0.86 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.79 | -1.25 | 2.64 | 1.42 | 1.20 | 0.99 | 0.98 | 1.24 | 1.44 | 0.99 | 1.24 | 1.76 | -0.33 | -0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.35 | -0.75 | 2.04 | 1.89 | 1.44 | 1.02 | 1.00 | 1.27 | 1.57 | 1.82 | 0.00 | -0.59 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.28 | -0.78 | -0.53 | 0.54 | 2.23 | 2.32 | 0.97 | 0.86 | -0.71 | -0.48 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.06 | -0.21 | -0.54 | -0.95 | -0.99 | -0.70 | -0.54 | -0.14 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . Borda . kernel = torch.tensor( [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype = torch.float ) um_seis_bordado = F.conv2d(um_seis.view(1,1,28,28), kernel.view(1,1,3,3), padding=1) (pd.DataFrame(um_seis_bordado.view(28,28)) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = 0) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.41 | -1.40 | -2.11 | -1.74 | -0.75 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.05 | -1.30 | 0.39 | 4.01 | 2.39 | -1.70 | -0.33 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.33 | -2.11 | 2.05 | 1.22 | 3.02 | -0.15 | -0.33 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.76 | -1.11 | 2.35 | 0.90 | 1.55 | -2.51 | -0.29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.37 | -2.05 | -1.06 | 1.29 | 1.69 | -0.15 | -1.28 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.01 | -1.05 | -0.09 | 2.28 | 1.05 | 2.95 | -2.59 | -0.50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.47 | -2.40 | 0.55 | 1.14 | 1.60 | 0.09 | -1.60 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.33 | -1.80 | -0.28 | 1.89 | 1.40 | 2.13 | -1.90 | -0.57 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.95 | -0.40 | 2.57 | 1.23 | 2.04 | -2.13 | -1.04 | -0.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -1.85 | 0.71 | 1.14 | 1.42 | -1.49 | -1.66 | -0.13 | 0.00 | 0.00 | -0.37 | -0.62 | -0.62 | -0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.20 | -2.67 | 2.46 | 0.52 | 2.09 | -1.38 | -0.87 | 0.00 | -0.14 | -1.01 | -2.38 | -0.12 | -1.09 | -1.98 | -0.75 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.56 | -1.61 | 2.18 | 0.69 | 2.80 | -0.78 | -0.52 | -0.35 | -1.35 | -1.94 | 2.62 | 2.49 | 2.65 | 1.78 | -2.06 | -0.68 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.93 | -0.58 | 2.05 | 1.16 | -0.43 | -2.10 | -0.37 | -1.35 | -0.19 | 1.67 | 1.31 | 0.36 | 0.51 | 1.67 | 1.28 | -1.82 | -0.20 | 0.00 | 0.00 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -1.46 | -1.15 | 1.49 | 1.75 | -0.97 | -1.24 | -0.95 | -2.03 | 1.84 | 1.27 | 1.69 | -0.34 | 1.06 | 0.53 | 2.45 | -1.27 | -0.47 | 0.00 | 0.00 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.25 | -2.32 | 1.31 | 0.96 | 2.32 | -0.40 | -1.49 | -2.67 | 1.89 | 1.75 | 1.91 | -0.35 | -5.89 | 0.67 | 0.56 | 2.88 | -0.60 | -0.49 | 0.00 | 0.00 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.25 | -0.51 | 3.01 | 0.59 | 1.98 | -3.28 | -2.48 | 1.82 | 1.44 | 1.86 | -1.82 | -3.22 | 1.40 | 1.16 | 1.35 | 0.55 | -1.85 | -0.29 | 0.00 | 0.00 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.25 | -2.70 | 1.77 | 0.79 | 1.79 | -4.33 | -1.70 | 2.16 | 0.63 | 1.25 | -5.09 | -0.58 | 1.49 | 0.53 | 2.16 | -1.96 | -0.80 | -0.02 | 0.00 | 0.00 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -1.48 | 2.48 | 1.63 | 2.32 | -0.57 | 0.20 | 0.88 | 0.34 | -0.31 | -1.87 | 1.87 | 0.56 | 1.18 | 1.38 | -2.06 | -0.16 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.66 | -1.68 | -2.75 | 3.18 | 1.42 | 0.65 | 0.22 | 0.22 | 0.71 | 0.68 | 0.59 | 0.90 | 1.82 | -0.92 | -0.95 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.13 | -1.34 | -1.27 | 2.42 | 2.29 | 1.27 | 0.48 | 0.35 | 0.73 | 1.73 | 1.99 | -0.33 | -1.46 | -0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.22 | -1.14 | -1.81 | -1.74 | 0.17 | 3.09 | 3.32 | 1.09 | 0.63 | -1.63 | -1.34 | -0.35 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.06 | -0.27 | -0.81 | -1.70 | -2.48 | -2.64 | -2.23 | -1.37 | -0.67 | -0.14 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . (pd.DataFrame(um_seis.view(28,28)) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = 0) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.41 | 0.99 | 0.71 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.84 | 0.99 | 0.98 | 0.29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.27 | 0.99 | 0.99 | 0.78 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.44 | 0.99 | 0.99 | 0.45 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.37 | 0.96 | 0.99 | 0.99 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.67 | 0.99 | 0.99 | 0.50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.46 | 0.99 | 0.99 | 0.84 | 0.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.33 | 0.99 | 0.99 | 0.91 | 0.13 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.62 | 0.99 | 0.99 | 0.35 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.90 | 0.99 | 0.99 | 0.27 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.37 | 0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.20 | 0.95 | 1.00 | 0.98 | 0.25 | 0.00 | 0.00 | 0.00 | 0.14 | 0.87 | 0.99 | 0.99 | 0.71 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.36 | 0.99 | 0.99 | 0.52 | 0.00 | 0.00 | 0.00 | 0.35 | 0.86 | 0.99 | 0.99 | 0.99 | 0.99 | 0.64 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.36 | 0.99 | 0.99 | 0.36 | 0.00 | 0.00 | 0.13 | 0.87 | 0.99 | 0.99 | 0.76 | 0.96 | 0.99 | 0.94 | 0.20 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.73 | 0.99 | 0.99 | 0.36 | 0.00 | 0.00 | 0.82 | 0.99 | 0.99 | 0.60 | 0.04 | 0.90 | 0.99 | 0.99 | 0.27 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.25 | 0.97 | 0.99 | 0.91 | 0.03 | 0.00 | 0.74 | 0.98 | 0.99 | 0.42 | 0.15 | 0.82 | 0.98 | 0.99 | 0.62 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.82 | 0.99 | 0.91 | 0.04 | 0.32 | 0.99 | 0.99 | 0.87 | 0.02 | 0.54 | 0.99 | 0.99 | 0.95 | 0.16 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.66 | 0.89 | 0.99 | 0.68 | 0.77 | 0.99 | 0.99 | 0.76 | 0.53 | 0.99 | 0.99 | 0.99 | 0.70 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.13 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.87 | 0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.22 | 0.86 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.99 | 0.85 | 0.35 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 | 0.21 | 0.54 | 0.95 | 0.99 | 0.70 | 0.54 | 0.14 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . ( 0.99*-1 + 0.99*-1 + 0.99*-1 + 0.94*0 + 0.99*0 + 0.62*0 + 0.2*1 + 0.27*1 + 0.02*1 ) . -2.4799999999999995 . ( 0*-1 + 0.25*-1 + 0*-1 + 0.73*0 + 0.97*0 + 0.82*0 + 0.99*1 + 0.99*1 + 0.99*1 ) . 2.7199999999999998 . kernel = torch.tensor( [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], dtype = torch.float ) um_seis_bordado = F.conv2d(um_seis.view(1,1,28,28), kernel.view(1,1,3,3), padding=1) (pd.DataFrame(um_seis_bordado.view(28,28)) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = -1) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.41 | 0.99 | 0.30 | -0.96 | -0.71 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 1.25 | 1.93 | 0.44 | -1.65 | -1.69 | -0.33 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.33 | 2.24 | 2.64 | 0.23 | -2.64 | -2.47 | -0.33 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.76 | 2.82 | 2.20 | -0.61 | -2.67 | -2.21 | -0.29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.37 | 1.68 | 2.59 | 1.29 | -1.69 | -2.96 | -1.28 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 1.04 | 2.38 | 1.93 | 0.08 | -2.47 | -2.48 | -0.50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.47 | 2.03 | 2.47 | 0.79 | -1.38 | -2.78 | -1.56 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.33 | 1.47 | 2.31 | 1.43 | -0.68 | -2.32 | -1.96 | -0.57 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.95 | 2.45 | 2.02 | -0.20 | -2.00 | -2.18 | -0.97 | -0.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.85 | 2.98 | 1.11 | -1.44 | -2.84 | -1.53 | -0.13 | 0.00 | 0.00 | 0.37 | 0.25 | -0.37 | -0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.20 | 2.47 | 2.78 | 0.48 | -2.12 | -2.95 | -0.87 | 0.00 | 0.14 | 0.87 | 1.22 | 0.37 | -0.65 | -1.20 | -0.71 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.56 | 2.84 | 2.42 | -0.36 | -2.47 | -2.48 | -0.52 | 0.35 | 1.00 | 1.51 | 1.35 | 0.37 | -0.65 | -1.55 | -1.70 | -0.68 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.93 | 2.93 | 2.05 | -1.07 | -2.74 | -1.85 | -0.12 | 1.22 | 1.86 | 1.63 | 0.76 | 0.10 | -0.05 | -1.33 | -2.50 | -1.62 | -0.20 | 0.00 | 0.00 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.46 | 2.96 | 1.51 | -1.73 | -2.98 | -1.24 | 0.95 | 2.21 | 1.89 | 0.36 | -1.05 | 0.28 | 1.19 | -0.28 | -2.51 | -2.57 | -0.47 | 0.00 | 0.00 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.25 | 2.07 | 2.71 | 0.82 | -2.21 | -2.89 | -0.01 | 1.93 | 2.11 | 0.46 | -1.12 | -0.77 | 1.11 | 1.35 | -0.30 | -2.49 | -2.55 | -0.49 | 0.00 | 0.00 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.25 | 2.52 | 2.72 | 0.29 | -2.54 | -2.50 | 1.31 | 2.48 | 1.12 | -1.37 | -1.56 | 0.43 | 1.58 | 1.08 | -1.11 | -2.64 | -1.77 | -0.29 | 0.00 | 0.00 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.25 | 2.45 | 2.62 | 0.36 | -2.13 | -1.73 | 1.98 | 1.87 | -0.11 | -1.99 | -0.94 | 1.84 | 1.28 | -0.16 | -2.18 | -2.62 | -0.78 | -0.02 | 0.00 | 0.00 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.48 | 2.01 | 1.42 | -0.31 | -0.82 | 1.27 | 0.89 | -0.35 | -1.43 | -0.10 | 1.43 | 0.33 | -1.07 | -2.68 | -1.89 | -0.16 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.66 | 1.02 | 1.54 | 1.51 | 0.55 | 0.44 | 0.22 | -0.22 | -0.45 | 0.22 | 0.32 | -0.76 | -1.89 | -2.20 | -0.95 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.13 | 1.21 | 1.78 | 0.98 | 0.61 | 0.74 | 0.46 | -0.25 | -0.46 | -0.69 | -1.30 | -1.73 | -1.21 | -0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.22 | 0.92 | 0.98 | 0.61 | 0.74 | 0.46 | -0.25 | -0.46 | -0.69 | -1.18 | -0.99 | -0.35 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 | 0.21 | 0.48 | 0.74 | 0.45 | -0.25 | -0.45 | -0.56 | -0.54 | -0.14 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . kernel = torch.tensor( [[-1, -1, -1], [ 0, 0, 0], [ 1, 1, 1]], dtype = torch.float ) um_seis_bordado = F.conv2d(um_seis.view(1,1,28,28), kernel.view(1,1,3,3), padding=1) (pd.DataFrame(um_seis_bordado.view(28,28)) .style.set_properties( **{&#39;font-size&#39;:&#39;6pt&#39;, &#39;width&#39;: &#39;18px&#39;, &#39;text-align&#39;: &#39;center&#39;}) .background_gradient(&#39;Greys_r&#39;, vmax = 1, vmin = -1) .format(&#39;{:.2f}&#39;) ) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.41 | 1.40 | 2.11 | 1.74 | 0.75 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.89 | 1.88 | 2.81 | 2.26 | 1.27 | 0.29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.27 | 0.85 | 0.85 | 0.65 | 0.03 | 0.04 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 4 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.39 | 0.53 | 0.53 | -0.38 | -0.82 | -0.82 | -0.29 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 5 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.37 | 1.06 | 1.06 | 0.69 | -0.74 | -0.74 | -0.74 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 6 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.68 | 1.23 | 1.22 | 0.07 | -0.94 | -0.94 | -0.45 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 7 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.46 | 1.08 | 1.10 | 0.49 | -1.04 | -1.11 | -0.96 | -0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 8 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.33 | 1.31 | 1.64 | 1.22 | -0.62 | -1.44 | -1.36 | -0.50 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 9 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.62 | 1.15 | 1.15 | -0.11 | -1.48 | -1.55 | -0.91 | -0.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 10 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.57 | 0.57 | 0.57 | -0.64 | -0.77 | -0.77 | -0.13 | 0.00 | 0.00 | 0.37 | 0.62 | 0.62 | 0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 11 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.20 | 0.54 | 0.55 | 0.33 | -0.11 | -0.11 | -0.10 | 0.00 | 0.14 | 1.01 | 2.00 | 2.85 | 2.69 | 1.74 | 0.75 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 12 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.36 | 0.45 | 0.45 | -0.38 | -0.74 | -0.74 | -0.27 | 0.35 | 1.21 | 2.20 | 2.46 | 2.35 | 2.35 | 2.38 | 1.64 | 0.64 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 13 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.16 | 0.20 | 0.19 | -0.59 | -0.87 | -0.86 | -0.12 | 1.00 | 1.84 | 1.84 | 0.74 | -0.14 | 0.03 | 1.16 | 1.38 | 1.10 | 0.20 | 0.00 | 0.00 | 0.00 | 0.00 | . 14 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.37 | 0.37 | 0.37 | -0.16 | -0.16 | -0.16 | 0.82 | 1.47 | 1.60 | 0.38 | -1.21 | -1.43 | -1.04 | 0.26 | 0.62 | 0.62 | 0.27 | 0.00 | 0.00 | 0.00 | 0.00 | . 15 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.25 | 0.86 | 0.86 | 0.53 | -0.41 | -0.41 | 0.41 | 1.60 | 1.72 | 0.40 | -1.29 | -1.35 | -0.76 | 0.07 | -0.31 | -0.50 | -0.50 | -0.17 | 0.00 | 0.00 | 0.00 | 0.00 | . 16 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.08 | 0.09 | 0.01 | -0.40 | -0.09 | 0.98 | 1.48 | 1.04 | -0.93 | -1.15 | -0.07 | 0.99 | 1.00 | -0.78 | -1.14 | -1.09 | -0.27 | 0.00 | 0.00 | 0.00 | 0.00 | . 17 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.25 | -0.56 | -0.66 | -0.33 | 0.63 | 1.50 | 1.67 | 1.03 | 0.02 | -0.11 | 0.72 | 1.12 | 1.01 | -0.12 | -0.90 | -0.93 | -0.64 | -0.02 | 0.00 | 0.00 | 0.00 | 0.00 | . 18 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.82 | -1.68 | -1.60 | 0.16 | 1.70 | 1.62 | 0.66 | 0.12 | 1.09 | 1.54 | 1.41 | 0.32 | -0.83 | -0.99 | -0.87 | -0.16 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 19 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.66 | -1.55 | -2.33 | -1.48 | -0.38 | 0.40 | 0.22 | 0.24 | 0.69 | 0.69 | 0.32 | -0.78 | -1.47 | -1.34 | -0.70 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 20 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.13 | -1.12 | -2.05 | -2.70 | -2.16 | -1.27 | -0.49 | -0.33 | -0.74 | -1.59 | -2.17 | -1.96 | -1.11 | -0.25 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 21 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.22 | -1.08 | -2.07 | -2.84 | -2.96 | -2.97 | -2.97 | -2.97 | -2.83 | -2.19 | -1.20 | -0.35 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 22 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.06 | -0.27 | -0.81 | -1.70 | -2.48 | -2.64 | -2.23 | -1.37 | -0.67 | -0.14 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 23 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 24 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 26 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 27 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . kernel = torch.tensor( [[-1, -1, -1], [ 0, 0, 0], [ 1, 1, 1]], dtype = torch.float ) plt.imshow(kernel, cmap = &#39;bwr&#39;); . CNN . Path: /root/.fastai/data/mnist_png . tarining . 0 | 1 | 2 | ... | . | testing . 0 | 1 | 2 | ... | . | . path.ls() . (#2) [Path(&#39;/root/.fastai/data/mnist_png/training&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing&#39;)] . dls = DataBlock( blocks = (ImageBlock(cls=PILImageBW), CategoryBlock), get_items = get_image_files, splitter = GrandparentSplitter(train_name=&#39;training&#39;, valid_name=&#39;testing&#39;), get_y = parent_label ).dataloaders(path, bs = 128, num_workers = 10) dls.show_batch() . learn = Learner(dls, xresnet18(c_in = 1), metrics = error_rate) . learn.fit_one_cycle(5) . epoch train_loss valid_loss error_rate time . 0 | 0.095464 | 0.093186 | 0.029300 | 00:17 | . 1 | 0.056486 | 0.045684 | 0.015800 | 00:17 | . 2 | 0.028911 | 0.031794 | 0.010800 | 00:16 | . 3 | 0.010618 | 0.018615 | 0.006500 | 00:17 | . 4 | 0.004242 | 0.018373 | 0.005600 | 00:17 | . /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . model = learn.model . kernels = model[0][0].weight.cpu().detach().numpy() kernels . kernels.shape . (32, 1, 3, 3) . cols = 8 rows = 4 fig = plt.figure(figsize=(cols, 1.2*rows)) for i in range(rows*cols): img = kernels[i, 0] fig.add_subplot(rows, cols, i + 1) plt.imshow(img, cmap=&#39;bwr&#39;) plt.title(f&#39;K{i+1}&#39;) plt.axis(&#39;off&#39;) plt.show() . learn.summary() . XResNet (Input shape: 128) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 128 x 32 x 14 x 14 Conv2d 288 True BatchNorm2d 64 True ReLU Conv2d 9216 True BatchNorm2d 64 True ReLU ____________________________________________________________________________ 128 x 64 x 14 x 14 Conv2d 18432 True BatchNorm2d 128 True ReLU MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Sequential ReLU Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Sequential ReLU ____________________________________________________________________________ 128 x 128 x 4 x 4 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 128 x 128 x 4 x 4 Conv2d 8192 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Sequential ReLU ____________________________________________________________________________ 128 x 256 x 2 x 2 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 128 x 256 x 2 x 2 Conv2d 32768 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Sequential ReLU ____________________________________________________________________________ 128 x 512 x 1 x 1 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 128 x 512 x 1 x 1 Conv2d 131072 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Sequential ReLU AdaptiveAvgPool2d Flatten Dropout ____________________________________________________________________________ 128 x 1000 Linear 513000 True ____________________________________________________________________________ Total params: 11,708,168 Total trainable params: 11,708,168 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f0e15897050&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . cols = 8 rows = 4 fig = plt.figure(figsize=(cols, 1.2*rows)) for i in range(rows*cols): img = F.conv2d(um_seis.view(1,1,28,28), tensor(kernels[i:i+1]), stride=1, padding=1).view(28,28) fig.add_subplot(rows, cols, i + 1) plt.imshow(img, cmap=&#39;bwr&#39;) plt.title(f&#39;K{i+1}&#39;) plt.axis(&#39;off&#39;) plt.show() .",
            "url": "https://opassos.github.io/blog/tutorial/cnn/2021/10/16/Como-uma-IA-encherga-o-Mundo.html",
            "relUrl": "/tutorial/cnn/2021/10/16/Como-uma-IA-encherga-o-Mundo.html",
            "date": " • Oct 16, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Tutorial Gatos vs Cachorros 🙀x🐶",
            "content": ". from IPython.display import YouTubeVideo YouTubeVideo(&#39;-Sw87Dyh3Kss&#39;) . https://youtu.be/Sw87Dyh3Kss . Setup . !nvidia-smi . Fri Oct 15 16:26:50 2021 +--+ | NVIDIA-SMI 470.74 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla V100-SXM2... Off | 00000000:00:04.0 Off | 0 | | N/A 35C P0 23W / 300W | 0MiB / 16160MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . !pip install -Uqqq fastai . from fastai.vision.all import * . Baixando o dataset . L(dir(URLs)) . (#95) [&#39;ADULT_SAMPLE&#39;,&#39;AG_NEWS&#39;,&#39;AMAZON_REVIEWS&#39;,&#39;AMAZON_REVIEWS_POLARITY&#39;,&#39;BIWI_HEAD_POSE&#39;,&#39;BIWI_SAMPLE&#39;,&#39;CALTECH_101&#39;,&#39;CAMVID&#39;,&#39;CAMVID_TINY&#39;,&#39;CARS&#39;...] . URLs.PETS . &#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&#39; . path = untar_data(URLs.PETS)/&#39;images&#39; path . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;) . EDA (Explorando o Dataset) . files = get_image_files(path) . files . (#7390) [Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_204.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/boxer_165.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/basset_hound_125.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Bombay_185.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_92.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_47.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Siamese_31.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_74.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/beagle_157.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_176.jpg&#39;)...] . Image.open(path/&#39;pug_116.jpg&#39;) . Image.open(path/&#39;British_Shorthair_201.jpg&#39;) . Dataloader . def labeler(x): return &#39;Gato&#39; if x.name[0].isupper() else &#39;Cachorro&#39; . labeler(path/&#39;British_Shorthair_201.jpg&#39;) . &#39;Gato&#39; . labeler(path/&#39;pug_116.jpg&#39;) . &#39;Cachorro&#39; . files = get_image_files(path) files . (#7390) [Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_204.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/boxer_165.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/basset_hound_125.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Bombay_185.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_92.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_47.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/Siamese_31.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_74.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/beagle_157.jpg&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_176.jpg&#39;)...] . labeler(files[0]) . &#39;Gato&#39; . dblock = DataBlock( blocks = [ImageBlock, CategoryBlock], # blocos que formata os dados splitter = RandomSplitter(valid_pct = 0.2, seed = 42), get_items = get_image_files, # função que carrega os dados get_y = labeler, # get_x = ... item_tfms = Resize(224), ) dls = dblock.dataloaders(path, num_workers = 2, bs = 64) dls.show_batch() . xb, yb = dls.one_batch() . Learner . learn = cnn_learner(dls, resnet50, metrics=accuracy) . /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . Transfer Learning . learn.fine_tune(3) . epoch train_loss valid_loss accuracy time . 0 | 0.090986 | 0.022098 | 0.993911 | 00:51 | . epoch train_loss valid_loss accuracy time . 0 | 0.075581 | 0.028851 | 0.991204 | 00:52 | . 1 | 0.034629 | 0.010006 | 0.995940 | 00:51 | . 2 | 0.016202 | 0.002749 | 0.999323 | 00:51 | . 0.999323 ** 12 . 0.9919061815543482 . Verifica&#231;&#227;o . learn.show_results() . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . (996+1+1+480) / 7390 . 0.2 . Testando com uma nova imagem . import ipywidgets as widgets uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) label, _, probs = learn.predict(img) if label == &#39;Gato&#39;: print(f&#39;Isso é um {label} com {(probs[1].item()*100):.1f}% de certeza&#39;) else: print(f&#39;Isso é um {label} com {(probs[0].item()*100):.1f}% de certeza&#39;) img.to_thumb(300) . Isso é um Gato com 100.0% de certeza . Problemas a serem resolvidos . E se a foto for de um leão? | E se o cachorro for o Pluto? | E se tivermos gatos e cachorros na foto? | E se a foto for de uma cadeira? | . Data Augmentation . dblock = DataBlock( blocks = [ImageBlock, CategoryBlock], get_items = get_image_files, get_y = labeler, item_tfms = Resize(256), batch_tfms = aug_transforms(mult = 2, size = 224) ) dls = dblock.dataloaders(path, num_workers = 2) . dls.show_batch(unique = True) .",
            "url": "https://opassos.github.io/blog/tutorial/cnn/2021/10/15/Gatos-vs-Cachorros.html",
            "relUrl": "/tutorial/cnn/2021/10/15/Gatos-vs-Cachorros.html",
            "date": " • Oct 15, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Tutorial Yolov5",
            "content": "Entre no repositório github do yolov5: https://github.com/ultralytics/yolov5 . | Abra o notebook colab . | . Clique no ícone [ ] para rodar o código de setup | . Aceite o aviso de que o notebook não foi criado pela Google . | O ícone [ ] deve mudar para um símbolo de play . | E depois de ser executado ele se tornará um número [1] . | Na aba chamada ‘Inferència’ é onde você irá colocar o vídeo que você quer detectar objetos. Existem varias opções, mas a mais simples é usar diretamente um vídeo do YouTube. Para isso, substitua o texto `data/images/´ pelo link do video. ** No caso, eu escolhi o trailer do Homem-Aranha 3: https://www.youtube.com/watch?v=EZo8V4XgzPY **Remova também a segunda linha. . | Clique no botão de ‘Play’ e espere alguns minutos (depdendendo do tamanho do vídeo) . | Para baixar o video anotado, abra a arvore dos arquivos . | O arquivo se encontra em: yolov5/runs/detect/exp/video.mp4 . | Para baixar clique nos 3 pontos e depois em download . | Um circulo de progressão será mostrado ao lado do nome do arquivo, quando ele ficar totalmente laranja o download será iniciado. . | E é isso! .",
            "url": "https://opassos.github.io/blog/2021/09/22/Tutorial-YoloV5.html",
            "relUrl": "/2021/09/22/Tutorial-YoloV5.html",
            "date": " • Sep 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Olá Mundo",
            "content": "Miss&#227;o . Sou um apaixonado por Deep Learning e decidi juntar a minha experiência em sala de aula com essa paixão. Em minhas redes sociais vou te ensinar o caminho mais curto para se tornar um especialista em Machine Learning e se tornar um profissional desejado e mais bem pago. . Meus Links: . GitHub | Kaggle | LinkedIn | Instagram | YouTube | Facebook | .",
            "url": "https://opassos.github.io/blog/ol%C3%A1%20mundo/jupyter/2021/09/20/Ola-Mundo.html",
            "relUrl": "/ol%C3%A1%20mundo/jupyter/2021/09/20/Ola-Mundo.html",
            "date": " • Sep 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://opassos.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://opassos.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}